\lab{Conditioning and Stability}{Conditioning and Stability}
\objective{The \emph{condition number} of a function measures how sensitive that function is to changes in the input.
On the other hand, the \emph{stability} of an algorithm measures how accurately that algorithm computes the value of a function from exact input.
In this lab, we examine the conditioning of common linear algebra problems, including computing polynomial roots and matrix eigenvalues.
We also study several least squares algorithms to show that two algorithms for the same problem may not have the same level of stability.}
\label{lab:conditioning_stability}

%\begin{equation*}
%\mathlarger{ \mathlarger{ \mathlarger{f:X \rightarrow Y}}}
%\end{equation*}
%\begin{center} vs. \end{center}
%\begin{equation*}
%\mathlarger{ \mathlarger{ \mathlarger{\hat{f}:\hat{X} \rightarrow \hat{Y}}}}
%\end{equation*}

%\begin{eqnarray}
%\mathlarger{\mathlarger{\mathlarger{f:X \rightarrow Y}}}\\ \mathlarger{\mathlarger{\mathlarger{ f:\hat{X} \rightarrow \hat{Y} }}}\\
%\mathlarger{\mathlarger{\mathlarger{ \hat{f}:X \rightarrow Y }}}%\\
%%\mathlarger{\mathlarger{\mathlarger{ \hat{f}:\hat{X} \rightarrow \hat{Y} }}}
% \end{eqnarray}
%

\section*{Condition Numbers} % ================================================

The \emph{absolute condition number} of a function $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$ at a point $\x\in\mathbb{R}^m$ is defined by
\begin{equation}\label{eq:abs_cond}
\hat{\kappa} (\x) = \lim_{\delta \rightarrow 0^+} \sup_{\norm{\mathbf{h}} < \delta} { \frac{\norm{f(\x + \mathbf{h} ) - f(\x)}}{\norm{\mathbf{h}}} }.
\end{equation}

In other words, the absolute condition number of $f$ is the limit of the change in output over the change of input.
Similarly, the \emph{relative condition number} of $f$ is the limit of the \emph{relative} change in output over the \emph{relative} change in input:
\begin{equation}\label{eq:rel_cond}
\kappa(\x)
= \lim_{\delta \rightarrow 0^+} \sup_{\norm{\mathbf{h}} < \delta} \left({ \frac{\norm{f(\x + \mathbf{h} ) - f(\x)}}{\norm{f(\x)}} } \middle/ { \frac{\norm{\mathbf{h}}}{\norm{\x}} }\right)
= \frac{\norm{\x}}{\norm{f(\x)}} \hat{\kappa}(\x).
\end{equation}

When a function's condition number is large, it is called \emph{ill-conditioned}.
Small changes to the input of an ill-conditioned function may produce large changes in output.
In applications, it is important to know if a function is ill-conditioned because floating point representation almost always introduces input error.

The \emph{condition number of a matrix} $\kappa (A)=\|A\| \|A^{-1}\|$ is an upper bound on the condition number for many of the common problems associated with the matrix, such as solving the system $A\x =\b$.
If $A$ is square but not invertible, then $\kappa(A) = \infty$ by convention.
To compute $\kappa(A)$, we often use the matrix 2-norm, which is the largest singular value $\sigma_{\max}$ of $A$.
Then since if $\sigma$ is a singular value of $A$, $\frac{1}{\sigma}$ is a singular value of $A^{-1}$, we have
\begin{equation}\label{eq:matrix_cond}
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}},
\end{equation}
which is also a valid equation for non-square matrices.

\begin{problem}\label{matrix-cond} % Compute matrix condition number.
Write a function that accepts a matrix $A$ and computes its condition number using (\ref{eq:matrix_cond}).
Use \li{scipy.linalg.svd()}, \li{scipy.linalg.svdvals()}, or \li{np.linalg.svd()} to compute the singular values of $A$, and avoid computing $A^{-1}$.
If the smallest singular value is 0, return infinity (\li{np.inf}).

Test your function against \li{np.linalg.cond()}; you should expect the values to be within $10^{-7}$ of one another.
\end{problem}

For large matrices where taking the SVD is difficult, the exact condition number of a matrix cannot always be computed and therefore must be estimated.
Although not covered here, there exist many algorithms that can efficiently and accurately estimate the condition number of a matrix.

%\begin{example}[Condition of a System of Equations]
%Consider the system of equations
%\[ Ax = b \]
%for an $n \times n$ matrix $A$ and $n \times 1$ vectors $x$ and $b$. If we hold $A$ fixed, consider the problem of computing $b$ with respect to small changes in $x$. Alternatively, hold $A$ fixed and consider the problem of computing $x = A^{-1} b$ with respect to small changes in $b$. Finally, we may hold $b$ fixed and consider the problem of computing $x = A^{-1}b$ with respect to small changes in $A$.
%
%It turns out that each of these problems has the \emph{same} relative condition number. This number is
%\[
%\mathcal{K} (A) = \norm{A}_2 \norm{A^{-1}}_2
%\]
%
%This is called the \emph{condition number of $A$}.
%The condition number of a matrix can be computed using \li{numpy.linalg.cond()}.
%
%Notice that the condition number of a matrix cannot be less that $1$.
%It is almost always best to work with orthonormal matrices (or operations that can be mathematically represented by orthonormal matrices).
%Since orthonormal matrices have a norm of $1$, as do their inverses, these transformations have the best possible condition number.
%The low condition number of orthonormal matrices is one of the primary reasons that Householder reflections and Givens rotations are used in so many algorithms.
%\end{example}

\subsection*{Example: The Wilkinson Polynomial} % -----------------------------

Let $f:\mathbb{C}^{n+1} \rightarrow \mathbb{C}^n$ be the function that maps the sequence of coefficients $(a_1, \ldots, a_{n+1})$ to the roots of the polynomial $a_1x^n+a_2x^{n-1}+\ldots+a_nx+a_{n+1}$.
% In other words, this function describes the problem of finding the roots of a polynomial.
Finding the roots of polynomials is extremely ill-conditioned in general, so the condition number of $f$ is likely very large.

For instance, consider the Wilkinson polynomial.
\[
w(x) = \prod_{r=1}^{20}(x-r) = x^{20}-210x^{19}+20615x^{18}-\ldots
\]
Let $\tilde{w}(x)$ be $w(x)$ where the coefficient on $x^{19}$ is very slightly perturbed from $-210$ to $-210.0000001$.
Below, we compute and compare the roots of $\tilde{w}(x)$ and $w(x)$ using NumPy and SymPy.

\begin{comment}
\begin{lstlisting}
>>> import numpy as np
>>> from scipy import linalg as la
>>> w_coeffs = np.array([1, -210, 20615, -1256850, 53327946, -1672280820,
                    40171771630, -756111184500, 11310276995381,
                    -135585182899530, 1307535010540395,
                    -10142299865511450, 63030812099294896,
                    -311333643161390640, 1206647803780373360,
                    -3599979517947607200, 8037811822645051776,
                    -12870931245150988800, 13803759753640704000,
                    -8752948036761600000, 2432902008176640000])
>>> w_roots = np.arange(1, 21)
\end{lstlisting}
\end{comment}

\begin{lstlisting}
>>> import numpy as np
>>> import sympy as sy
>>> from scipy import linalg as la

# The roots of w are 1, 2, ..., 20.
>>> w_roots = np.arange(1, 21)

# Get the exact Wilkinson polynomial coefficients using SymPy.
>>> x, i = sy.symbols('x i')
>>> w, _ = sy.poly_from_expr(sy.product(x-i, (i, 1, 20)))
>>> w_coeffs = np.array(w.all_coeffs())

# Perturb one of the coefficients very slightly.
>>> perturb = np.zeros(21)
>>> perturb[1]=1e-7
>>> perturbed_coeffs = w_coeffs - perturb

# Use NumPy to compute the roots of the perturbed polynomial.
>>> perturbed_roots = np.roots(np.poly1d(perturbed_coeffs))
\end{lstlisting}

Below we plot the polynomials $w(x)$ and $\tilde{w}(x)$ and compare their roots in the complex plane.

\begin{figure}[H] % Original and pertubed Wilkinson polynomial + roots
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/perturbed_poly.pdf}
    \caption{The original and perturbed Wilkinson polynomials.
    They match for only about half of the domain.}
\end{subfigure}
\qquad
\begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/perturbed_roots.pdf}
    \caption{Roots of the original and pertubed Wilkinson polynomials.
    About half of the perturbed roots are imaginary.}
    \label{fig:pertubed-roots}
\end{subfigure}
\label{fig:wilkinsonpolynomial}
\end{figure}

From the figure, it's clear that a perturbation drastically changes the nature of the polynomial and its root.
To quantify the difference, we estimate the condition numbers in the $L^\infty$ norm.

\begin{lstlisting}
# Sort the roots to ensure that they are in the same order.
>>> w_roots = np.sort(w_roots)
>>> perturbed_roots = np.sort(perturbed_roots)

# Estimate the absolute condition number in the infinity norm.
>>> k = la.norm(perturbed_roots - w_roots, np.inf) / la.norm(perturb, np.inf)
>>> print(k)
28262391.3304

# Estimate the relative condition number in the infinity norm.
>>> k*la.norm(w_coeffs, np.inf) / la.norm(w_roots, np.inf)
1.95063629993970+25                     # This is huge!!
\end{lstlisting}

There are some caveats to this example.
First, when we compute the quotients in \eqref{eq:abs_cond} and \eqref{eq:rel_cond} for a fixed $\mathbf{h}$, we are only \emph{approximating} the condition number.
The actual condition number is the limit of such quotients.
We hope that when $||\mathbf{h}||$ is small, a random quotient is at least the same order of magnitude as the limit, but we have no way to be sure.

Second, this example assumes that NumPy's root-finding algorithm is \emph{stable}, so that the difference between the roots of \li{w_coeffs} and \li{perturbed_coeffs} is due to the difference in coefficients, and not the difference in roots.
We will return to this issue in the next section.

\begin{problem}\label{prob:wilk}
Write a function that carries out the following experiment 100 times.
\begin{enumerate}
\item Randomly perturb the true coefficients by replacing each coefficient $a_i$ with $a_i*r_i$, where $r_i$ is drawn from a normal distribution centered at 1 with standard deviation $10^{-10}$ (use \li{np.random.normal()}).
\item Plot the perturbed roots as small points in the complex plane.
That is, plot the real part of the coefficients on the $x$-axis and the imaginary part on the $y$-axis.
Plot on the same figure in each experiment.
\item Compute the absolute and relative condition numbers with the $L^{\infty}$ norm.
\end{enumerate}
Plot the roots of the unperturbed Wilkinson polynomial with the perturbed roots.
Your final plot should resemble Figure \ref{fig:wilkinsonpolynomial_many}.
Finally, return the average computed absolute and relative condition numbers.

\begin{figure}[H]
\includegraphics[width=.7\linewidth]{figures/wilkinson_prob_solution.pdf}
\caption{
This figure replicates Figure 12.1 on p. 93 of \emph{Numerical Linear Algebra} by Lloyd N. Trefethen and David Bau III.}
\label{fig:wilkinsonpolynomial_many}
\end{figure}
\end{problem}

%Notes on Problem 1:
% - Also "compute" the roots of the wilkinson poly. They are close but not quite right. The difference is not visible on the graph. See that is probably useful.

% In introductory discussion, compute the condition number of the problem. Explain why we use the infinity norm.

\subsection*{Example: Calculating Eigenvalues} % ------------------------------

Let $f:\mathbb{C}^{n^2} \rightarrow \mathbb{C}^n$ be the function that maps an $n \times n$ matrix to its $n$ eigenvalues.
This problem is well-conditioned for symmetric matrices, but can be extremely ill-conditioned for non-symmetric matrices.
Let $A$ be an $n\times n$ matrix and let $\boldsymbol{\lambda}$ be the vector of the $n$ eigenvalues of $A$.
If $\tilde{A} = A + H$ and $\tilde{\boldsymbol{\lambda}}$ are a perturbation of $A$ and its eigenvalues, then the condition numbers of $f$ are
\begin{equation}
\hat{\kappa}(A) = \frac{\|\boldsymbol{\lambda} - \tilde{\boldsymbol{\lambda}}\|}{\|H\|},
\qquad
\kappa(A) = \frac{\|A\|}{\|\boldsymbol{\lambda}\|}\hat{\kappa}(A).
\label{eq:eig-condition-numbers}
\end{equation}

\begin{problem}\label{prob:eigenvalue} % Eigenvalue experiments
Write a function that accepts a matrix $A$ and estimates the condition number of the eigenvalue problem using (\ref{eq:eig-condition-numbers}).
For the perturbation $H$, use arandom complex matrices with entries drawn from a normal distribution.
\begin{lstlisting}
H = np.random.normal(0, 1e-10, A.shape) + np.random.normal(
                                                    0, 1e-10, A.shape)*1j
\end{lstlisting}

Use the 2-norm for both the vector and matrix norms.
Return the absolute and relative condition numbers.
\label{prob:eig-condit}
\end{problem}

\begin{problem}\label{prob:plot_eigen}
Write a function that accepts minimal and maximal values for $x$ and $y$ as well as a resolution parameter \li{res}.
Use your function from Problem \ref{prob:eig-condit} to compute the relative condition number of the eigenvalue for the 2x2 matrix
\[
\left[\begin{array}{cc}
1 &  x\\
y & 1\end{array}\right]
\]
at every point in the domain $[x_{min}\ x_{max}]\times [y_{min}\ y_{max}]$, where each axis is partitioned into \li{res} points.
Plots these estimated relative condition numbers using \li{plt.pcolormesh()}.
With \li{res}=200, your plot should similar to the following figure.

\begin{figure}[H]
    \includegraphics[width=.7\linewidth]{figures/eigenvalue_conditioning.png}
\end{figure}
\end{problem}

\section*{Stability of an Algorithm} % ========================================

The stability of an algorithm is measured by the error in its output.
Suppose we have some algorithm to compute $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$.
Let $\tilde f(\x)$ represent the value computed by the algorithm given an input $\x$.
Then the \emph{forward error} of $f$ at $\x$ is $||f(\x)-\tilde f(\x)||$, and the \emph{relative forward error} of $f$ at $\x$ is
\[
\frac{||f(\x)-\tilde f(\x)||}{||f(\x)||}.
\]
An algorithm is \emph{stable} if its relative forward error is small.

As an example, let us examine the stability of NumPy's root finding algorithm that we used to investigate the Wilkinson polynomial.
We know the exact roots of $w(x)$, and we can also compute these roots using NumPy's \li{np.roots()} function.

\begin{lstlisting}
>>> roots = np.arange(1,21)
 # w_coeffs is an array with the coefficients of the Wilkinson polynomial.
>>> computed_roots = np.roots(np.poly1d(w_coeffs))

# Sort the roots for comparison.
>>> roots = np.sort(roots)
>>> computed_roots = np.sort(computed_roots)

# Compute the forward error.
>>> forward_error = la.norm(roots-computed_roots)
>>> print(forward_error)
0.020612653126379665

# Compute the relative forward error.
>>> forward_error / la.norm(roots)
0.00038476268486104599                              # Nice and small.
\end{lstlisting}

This analysis suggests that questions of stability did not interfere too much with our experiments in Problem \ref{prob:wilk}.

\subsection*{Example: Least Squares} % ----------------------------------------
% An example of a problem with stable and unstable algorithms is the least squares problem.

The least squares problem is to find the $\x$ that minimizes $\norm{A\x -\b}_2$ for fixed $A$ and $\b$.
It can be shown that an equivalent problem is finding the solution of $A\trp A\x =A\trp \b$, called the \emph{normal equations}.
We will consider two different algorithms that solve this problem.

\begin{enumerate}
\item Invert the matrix $A\trp A$ and then right multiply by $A\trp \b$ to solve the normal equations.
Although this approach seems intuitive, it is actually highly unstable and can return an answer with a very large forward error.

\item
Use the QR-decomposition, factoring the $m\times n$ matrix $A$ of rank $n\geq m$, where $Q$ has orthonomral columns and $R$ is upper triangular.
It can also be shown that the solution of $R \x = Q\b$ is equivalent to solving the least squares problem.
This algorithm has the advantage of being stable.
\end{enumerate}

A common application of least squares is polynomial approximation.
Given a set of data points $\{(x_k, y_k)\}_{k=1}^m$ we would like to find the set of coefficients $\{c_k\}_{k=1}^n$ such that
\[
y_k = c_n x_k^n + c_{n-1} x_k^{n-1} + \cdots + c_2 x_k^2 + c_1 x_k + c_0
\]
for all $k$.
Since this system will typically be over-determined, there will not be a set of coefficients that exactly satisfies this equation for all $k$.
A common approach is to use least squares to pick the set of coefficients that minimizes the $L^2$ error of the system of equations.

\begin{lstlisting}
>>> from scipy import linalg as la

# Use least squares to approximate sin(x) with a five-degree polynomial.
>>> x = np.linspace(0, 6, 10)           # The x-values of the data.
>>> b = np.sin(x) + .2*np.random.randn(10) # The y-values of the data (noisy).
>>> A = np.vander(x, 6)                 # Set up the matrix of data values.
>>> coeffs = la.lstsq(A, b)[0]          # Get the polynomial coefficients.

>>> domain = np.linspace(0, 6, 100)     # Define a finer domain for plotting.
>>> plt.plot(x, b, 'k*')
>>> plt.plot(domain, np.sin(domain))
>>> plt.plot(domain, np.poly1d(coeffs)(domain))
\end{lstlisting}

\begin{problem}\label{problem:lstsqr}
Write two functions that solve the least squares problem using the normal equation.
One of your functions should solve the problem directly using \li{np.linalg.inv()} while the other should use a more stable algorithm, such as the QR-decomposition.
The functions \li{la.qr()} and \li{la.solve_triangular()} may be helpful when implementing the QR-decomposition.
Both of your functions should accept a matrix $A$ and a vector $\b$.

Approximate the data from \texttt{stability\_data.npy} on the interval $[0,\ 1]$ with a degree-fourteen polynomial using two approaches to get the least squares solution:
\begin{enumerate}
\item Use \li{la.inv()} and the normal equations: $\x = (A\trp A)^{-1}A\trp \b$.
\item Use \li{la.qr()} and \li{la.solve_triangular()} to solve the system $R\x = Q\b$.
\end{enumerate}
Plot both of your solutions with the data points for comparision.
Return the error $\norm{A\x-\mathbf{b}}_2$ of both approximations.
Your plot should look similar to the following figure.

\begin{figure}[H] % TODO: do they really need this figure?
    \centering
    \includegraphics[width=.7\textwidth]{figures/lstsq_stability.pdf}
\end{figure}

\end{problem}

\subsection*{Catastrophic Cancellation} % -------------------------------------

A common cause of instability in algorithms is catastrophic cancellation.
\emph{Catastrophic cancellation} is the term for when a computer takes the difference of two very similar numbers, and the result is stored with a small number of significant digits.
Because of the way computers store and perform arithmetic on numbers, future computations can amplify a catastrophic cancellation into a large error.

You are at risk for catastrophic cancellation whenever you subtract floats or large integers that are very close to each other.
You can avoid this problem by either rewriting your program to not use subtraction, or by increasing the number of significant digits that your computer tracks.

Here is an example of catastrophic cancellation.
Suppose we wish to compute $\sqrt{a}-\sqrt{b}$. We can either do this subtraction directly or perform the equivalent division
\[
\sqrt{a}-\sqrt{b} = (\sqrt{a}-\sqrt{b})\frac{\sqrt{a}+\sqrt{b}}{\sqrt{a}+\sqrt{b}} = \frac{a-b}{\sqrt{a}+\sqrt{b}}.
\]

We will perform this computation both ways in NumPy with $a=10^{20}+1$ and $b=10^{20}$.
\begin{lstlisting}
>>> from math import sqrt           # np.sqrt() fails for very large numbers

>>> a = 10**20+1
>>> b = 10**20
>>> sqrt(a)- sqrt(b)
0.0
>>> (a-b) / (np.sqrt(a)+np.sqrt(b))
5e-11
\end{lstlisting}
Since $a \neq b$, $\sqrt{a}-\sqrt{b}$ should clearly be nonzero.

\begin{problem} % TODO: fix this problem.
Let $I(n) = \int_0^1 x^n e^{x - 1} dx$.
\begin{enumerate}
\item Prove that $0 \leq I(n) \leq 1$ for all $n$.
\item It can be shown that for $n>1$,
\[
I(n) = \left(-1\right)^{n} !n + \left(-1\right)^{n + 1} \frac{n!}{e}
\]
where $!n$ is the \emph{subfactorial} of $n$.
Use this formula to write a function that computes the integral.
\\ (Hint: Use SymPy's \li{subfactorial()} function.)
\item The actual values of $I(n)$ for many values of $n$ are listed in the table below.
Use your function \li{integral()} to compute $I(n)$ for these same values of $n$, and create a table comparing the data.
How can you explain what is happening?
\end{enumerate}

\begin{center}
\begin{tabular}{|l|l|}
\hline
$n$  & Actual value of $I(n)$ \\
\hline
$1$  & $0.367879441171$ \\
$5$  & $0.145532940573$ \\
$10$ & $0.0838770701034$ \\
$15$ & $0.0590175408793$ \\
$20$ & $0.0455448840758$ \\
$25$ & $0.0370862144237$ \\
$30$ & $0.0312796739322$ \\
$35$ & $0.0270462894091$ \\
$40$ & $0.023822728669$ \\
$45$ & $0.0212860390856$ \\
$50$ & $0.0192377544343$ \\
\hline
\end{tabular}
\end{center}
\end{problem}

%\begin{table}
%\centering
%\begin{tabular}{|l|l|l|}
%\hline
%Integrand & Computed Value & Actual Value \\
%\hline
%$x^{1}e^{x}$: & $0.367879441171$ & $0.367879441171$ \\
%$x^{5}e^{x}$: & $0.145532940573$ & $0.145532940573$ \\
%$x^{10}e^{x}$: & $0.0838770701084$ & $0.0838770701034$ \\
%$x^{15}e^{x}$: & $0.0590209960938$ & $0.0590175408793$ \\
%$x^{20}e^{x}$: & $0.0$ & $0.0455448840758$ \\
%$x^{25}e^{x}$: & $1073741824.0$ & $0.0370862144237$ \\
%$x^{30}e^{x}$: & $-1.80143985095 \cdot 10^{16}$ & $0.0312796739322$ \\
%$x^{35}e^{x}$: & $6.04462909807 \cdot 10^{23}$ & $0.0270462894091$ \\
%$x^{40}e^{x}$: & $0.0$ & $0.023822728669$ \\
%$x^{45}e^{x}$: & $0.0$ & $0.0212860390856$ \\
%$x^{50}e^{x}$: & $1.46150163733 \cdot 10^{48}$ & $0.0192377544343$ \\
%\hline
%\end{tabular}
%\caption{Inaccuracy of values computed using an unstable algorithm.}
%\label{table:unstable_computation}
%\end{table}

% For this section to be meaningful, it needs more explanation of when each algorithm is stable.
\begin{comment}
The algorithms that we have studied to solve linear systems have different levels of stability.
LU decomposition (with pivoting) is usually good enough, but there are some pathological examples of matrices that cause it to break down.
QR decomposition (with pivoting) is generally considered to be a better option than the LU decomposition.
Solving a linear system using the SVD is even more stable than the QR decomposition.
(Pivoting is a modification that is commonly made to the LU decomposition and QR decomposition algorithms we have discussed in earlier labs to make them more stable.)
Unfortunately, in this case, the algorithms that are more stable are also slower.
The LU decomposition is used by \li{scipy.linalg.solve()}.
The SVD is used by \li{scipy.linalg.lstsq()}.
Here is some code that uses the QR decomposition of a matrix $A$ to solve the linear system $A x = b$ for $x$.
It uses a lower-level function included in SciPy to perform the back substitution required to solve this system.
\begin{lstlisting}
from scipy import linalg as la
from scipy.linalg.flapack import dtrtrs
def qr_solve(A, b):
    Q, R = la.qr(A)
    return dtrtrs(R.T, Q.T.dot(b), lower=1, trans=1)[0]
\end{lstlisting}
A solution using a pivoted QR decomposition would be better, but this will be good enough for demonstration purposes.

The following are routines that generate matrices designed to show the relative benefits of each of these algorithms.
\begin{lstlisting}
from numpy.random import rand

def bad_arr_1(n):
    """ Construct a specific pathological example
    that breaks LU decomposition. These examples
    are very rare, but they do exist.
    Strictly speaking, the condition number
    for this matrix isn't terribly bad. """
    A = - np.ones((n, n))
    A[:,:-1] = np.tril(A[:,:-1])
    np.fill_diagonal(A, 1)
    A[:,-1] = 1
    return A

def bad_arr_2(n, peturbation = 1E-8):
    """ Construct another matrix that is nearly singular
    by computing A.dot(A.T) for a matrix A that is
    not square and then adding some small changes
    so it is not exactly singular. """
    A = rand(n, n // 2)
    return A.dot(A.T) + peturbation * rand(n, n)
\end{lstlisting}
\end{example}

\begin{problem}
For each of the array creation routines above, plot the error $\norm{\text{solve}\left(A, A b\right) - b}$ of each of the methods mentioned above for solving a linear system.
Use a log-scaled $y$-axis.
What do you observe?
\end{problem}
\end{comment}
%\subsection*{Stable vs. Backward Stable Algorithms}

%\subsection*{Table}
%
%\begin{table}[h]
%\begin{tabular}{|l|l|l|}
%\hline \textbf{Well-Conditioned} & \textbf{Ill-Conditioned} & \textbf{Systems of Equations} \\
%{\parbox{0.3\textwidth}{\raggedleft
%            \begin{itemize}[leftmargin=*]
%                \item finding eigenvalues of a symmetric (or normal) matrix
%                \item calculating $e^x$ for relatively small values of $x$
%                \item calculating $\ln(x)$ for $x$ not close to $1$
%            \end{itemize} }}           &
%
%{\parbox{0.3\textwidth}{\raggedleft
%            \begin{itemize}[leftmargin=*]
%                \item calculating $x_1 - x_2$ when $x_1 \approx x_2$
%                \item computing roots of a polynomial, given the coefficients
%                \item computing eigenvalues of a non-symmetric matrix
%            \end{itemize} }}              &
%
%{\parbox{0.3\textwidth}{
%            \begin{itemize}[leftmargin=*]
%                 \item relative condition number is \[ \mathcal{K} = ||A|| ||A^{-1}|| \]
%            \end{itemize} }}   \\ \hline
%\end{tabular}
%\end{table}
%
%
%\begin{table}[h]
%\begin{tabular}{|l|l|}
%\hline \textbf{Stable} & \textbf{Unstable} \\
%{\parbox{0.45\textwidth}{\raggedleft
%            \begin{itemize}[leftmargin=*]
%                \item finding eigenvalues of a symmetric (or normal) matrix
%                \item calculating $e^x$ for relatively small values of $x$
%                \item calculating $\ln(x)$ for $x$ not close to $1$
%            \end{itemize} }}           &
%
%{\parbox{0.45\textwidth}{\raggedleft
%            \begin{itemize}[leftmargin=*]
%                \item calculating $x_1 - x_2$ when $x_1 \approx x_2$
%                \item computing roots of a polynomial, given the coefficients
%                \item computing eigenvalues of a non-symmetric matrix
%            \end{itemize} }}    \\ \hline
%\end{tabular}
%\end{table}
%
